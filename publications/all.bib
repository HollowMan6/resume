@misc{hu2025openrlhfeasytousescalablehighperformance,
      title={\href{https://arxiv.org/abs/2405.11143}{OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework}}, 
      author={Jian Hu and Xibin Wu and Wei Shen and Jason Klein Liu and Zilin Zhu and Weixun Wang and Songlin Jiang and Haoran Wang and Hao Chen and Bin Chen and Weikai Fang and Xianyu and Yu Cao and Haotian Xu and Yiming Liu},
      year={2025},
      _venue={Conference on Empirical Methods in Natural Language Processing (\textbf{EMNLP}) System Demonstrations},
      selected={true},
      abstract={Large Language Models (LLMs) fine-tuned via Reinforcement Learning from Human Feedback (RLHF)
      and Reinforcement Learning with Verifiable Rewards (RLVR) significantly improve the alignment of human-AI
      values and further raise the upper bound of AI capabilities, particularly in reasoning-intensive, long-context
      Chain-of-Thought (long-CoT) tasks. However, existing RLHF (or RLVR) frameworks commonly face challenges such
      as inference bottlenecks and complexity barriers, restricting their accessibility for newcomers. To bridge this
      gap, we introduce OpenRLHF, a user-friendly, scalable, and easy-to-learn open-source RLHF framework built upon
      Ray, vLLM, DeepSpeed, and HuggingFace Transformers, featuring a simplified design, clear code structure, and
      comprehensive documentation to facilitate entry for researchers and practitioners. Experimental results show
      that OpenRLHF achieves superior training efficiency with speedups ranging from 1.22x to 1.68x across different
      model sizes compared to state-of-the-art frameworks, while requiring significantly fewer lines of code for
      implementation. OpenRLHF is publicly available at \href{https://github.com/OpenRLHF/OpenRLHF}{this https URL},
      and has already been adopted by leading institutions to accelerate RLHF research and learning.
      }
}

@misc{yu2025quasarquantumassemblycode,
      title={\href{https://arxiv.org/abs/2510.00967}{QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL}}, 
      author={Cong Yu and Valter Uotila and Shilong Deng and Qingyuan Wu and Tuo Shi and Songlin Jiang and Lei You and Bo Zhao},
      year={2025},
      _venue={arXiv preprint arXiv:2510.00967},
      eprint={2510.00967},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.00967}, 
}

@misc{votingsystemwechatpay,
  title={\href{https://qikan.cqvip.com/Qikan/Article/Detail?id=7001708234}{Voting System Based on Wechat Pay}},
  author={Songlin Jiang},
  year={2019},
  _venue={Computer Engineering \& Software},
  selected={false},
  abstract={
    In contemporary society, there are problems existed in the online voting
    systems such as repeated fraudulent voting, data tampering, loss of voting data,
    and failure to verify the outcome of the voting data, and these problems causes
    inconvenience to the users. Meanwhile, the mobile pay systems represented by
    Wechat pay are distinguished due to its safety and convenience. If we combine
    the voting system with Wechat pay, a huge difference will be made. My system
    is based on Wechat pay. When you want to vote, you just need to pay a little,
    then a refund will give to you to ensure that the voting is real. The system can
    double check the voting record by checking the trading record saved on the Wechat
    Sever. As a result, the system provides users with a safe and fair environment to
    vote and it also reduces the developing cost.
  }
}
