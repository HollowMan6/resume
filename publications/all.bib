@misc{amos2023tutorial,
  title={Tutorial on amortized optimization},
  author={Brandon Amos},
  year={2023},
  url={https://arxiv.org/abs/2202.00665},
  _venue={Foundations and Trends in Machine Learning},
  codeurl={https://github.com/facebookresearch/amortized-optimization-tutorial},
  selected={true},
  abstract={
Optimization is a ubiquitous modeling tool and is often deployed
in settings which repeatedly solve similar instances
of the same problem. Amortized optimization methods
use learning to predict the solutions to problems in
these settings, exploiting the shared structure
between similar problem instances. These methods
have been crucial in variational inference and
reinforcement learning and are capable of solving
optimization problems many orders of magnitudes
times faster than traditional optimization methods
that do not use amortization. This tutorial presents
an introduction to the amortized optimization
foundations behind these advancements and overviews
their applications in variational inference, sparse
coding, gradient-based meta-learning, control,
reinforcement learning, convex optimization, optimal
transport, and deep equilibrium networks.
  }
}

@misc{amos2023amortizing,
  title={On amortizing convex conjugates for optimal transport},
  author={Brandon Amos},
  year={2023},
  url={https://arxiv.org/abs/2210.12153},
  codeurl={https://github.com/facebookresearch/w2ot},
  _venue={ICLR},
  selected={true},
  abstract={
    This paper focuses on computing the convex conjugate operation that
    arises when solving Euclidean Wasserstein-2 optimal
    transport problems. This conjugation, which is also
    referred to as the Legendre-Fenchel conjugate or
    c-transform, is considered difficult to compute and
    in practice, Wasserstein-2 methods are limited by
    not being able to exactly conjugate the dual
    potentials in continuous space. I show that
    combining amortized approximations to the conjugate
    with a solver for fine-tuning is computationally
    easy. This combination significantly improves the
    quality of transport maps learned for the
    Wasserstein-2 benchmark by Korotin et al. (2021) and
    is able to model many 2-dimensional couplings and
    flows considered in the literature.
  }
}

@misc{sambharya2023l2a,
  title={End-to-End Learning to Warm-Start for Real-Time Quadratic Optimization},
  author={Rajiv Sambharya and Georgina Hall and Brandon Amos and Bartolomeo Stellato},
  year={2023},
  url={https://arxiv.org/abs/2212.08260},
  codeurl={https://github.com/stellatogrp/l2ws},
  _venue={L4DC},
  abstract={
    First-order methods are widely used to solve convex quadratic programs
    (QPs) in real-time applications because of their low
    per-iteration cost. However, they can suffer from
    slow convergence to accurate solutions. In this
    paper, we present a framework which learns an
    effective warm-start for a popular first-order
    method in real-time applications, Douglas-Rachford
    (DR) splitting, across a family of parametric
    QPs. This framework consists of two modules: a
    feedforward neural network block, which takes as
    input the parameters of the QP and outputs a
    warm-start, and a block which performs a fixed
    number of iterations of DR splitting from this
    warm-start and outputs a candidate solution. A key
    feature of our framework is its ability to do
    end-to-end learning as we differentiate through the
    DR iterations. To illustrate the effectiveness of
    our method, we provide generalization bounds (based
    on Rademacher complexity) that improve with the
    number of training problems and number of iterations
    simultaneously. We further apply our method to three
    real-time applications and observe that, by learning
    good warm-starts, we are able to significantly
    reduce the number of iterations required to obtain
    high-quality solutions.
  }
}

@misc{amos2023meta,
  title={Meta Optimal Transport},
  author={Brandon Amos and Samuel Cohen and Giulia Luise and Ievgen Redko},
  year={2023},
  url={https://arxiv.org/abs/2206.05262},
  codeurl={https://github.com/facebookresearch/meta-ot},
  _venue={ICML},
  selected={true},
  abstract={
    We study the use of amortized optimization to predict optimal
    transport (OT) maps from the input measures, which
    we call Meta OT. This helps repeatedly solve similar
    OT problems between different measures by leveraging
    the knowledge and information present from past
    problems to rapidly predict and solve new
    problems. Otherwise, standard methods ignore the
    knowledge of the past solutions and suboptimally
    re-solve each problem from scratch. Meta OT models
    surpass the standard convergence rates of
    log-Sinkhorn solvers in the discrete setting and
    convex potentials in the continuous setting. We
    improve the computational time of standard OT
    solvers by multiple orders of magnitude in discrete
    and continuous transport settings between images,
    spherical data, and color palettes.
  }
}

@misc{pooladian2023multisample,
  title={Multisample Flow Matching: Straightening Flows with Minibatch Couplings},
  author={Aram-Alexandre Pooladian and Heli Ben-Hamu and Carles Domingo-Enrich and Brandon Amos and Yaron Lipman and Ricky T. Q. Chen},
  year={2023},
  _venue={ICML},
  url={https://arxiv.org/abs/2304.14772},
  abstract={
Simulation-free methods for training continuous-time generative models
construct probability paths that go between noise
distributions and individual data samples. Recent
works, such as Flow Matching, derived paths that are
optimal for each data sample. However, these
algorithms rely on independent data and noise
samples, and do not exploit underlying structure in
the data distribution for constructing probability
paths. We propose Multisample Flow Matching, a more
general framework that uses non-trivial couplings
between data and noise samples while satisfying the
correct marginal constraints. At very small overhead
costs, this generalization allows us to (i) reduce
gradient variance during training, (ii) obtain
straighter flows for the learned vector field, which
allows us to generate high-quality samples using
fewer function evaluations, and (iii) obtain
transport maps with lower cost in high dimensions,
which has applications beyond generative
modeling. Importantly, we do so in a completely
simulation-free manner with a simple minimization
objective. We show that our proposed methods improve
sample consistency on downsampled ImageNet data
sets, and lead to better low-cost sample generation.
  }
}

@misc{zheng2023semi,
  title = {Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories},
  author = {Zheng, Qinqing and Henaff, Mikael and Amos, Brandon and Grover, Aditya},
  year = {2023},
  url = {https://arxiv.org/abs/2210.06518},
  _venue={ICML},
  abstract={
Natural agents can effectively learn from multiple data sources that
differ in size, quality, and types of
measurements. We study this heterogeneity in the
context of offline reinforcement learning (RL) by
introducing a new, practically motivated
semi-supervised setting. Here, an agent has access
to two sets of trajectories: labelled trajectories
containing state, action, reward triplets at every
timestep, along with unlabelled trajectories that
contain only state and reward information. For this
setting, we develop a simple meta-algorithmic
pipeline that learns an inverse-dynamics model on
the labelled data to obtain proxy-labels for the
unlabelled data, followed by the use of any offline
RL algorithm on the true and proxy-labelled
trajectories. Empirically, we find this simple
pipeline to be highly successful -- on several D4RL
benchmarks, certain offline RL
algorithms can match the performance of variants
trained on a fully labeled dataset even when we
label only 10\% trajectories from the low return
regime. Finally, we perform a large-scale controlled
empirical study investigating the interplay of
data-centric properties of the labelled and
unlabelled datasets, with algorithmic design choices
(e.g., inverse dynamics, offline RL algorithm) to
identify general trends and best practices for
training RL agents on semi-supervised offline
datasets.
  }
}

@misc{bansal2023taskmet,
  title = {TaskMet: Task-Driven Metric Learning for Model Learning},
  author = {Dishank Bansal and Ricky T. Q. Chen and Mustafa Mukadam and Brandon Amos},
  year = {2023},
  _venue={NeurIPS},
  selected={true},
  url={https://differentiable.xyz/papers/paper_43.pdf},
  abstract={
    Deep learning models are often used with some downstream
    task. Models solely trained to achieve accurate
    predictions may struggle to perform well on
    the desired downstream tasks. We propose using the
    task's loss to learn a metric which parameterizes a
    loss to train the model.This approach does not alter
    the optimal prediction model itself, but rather
    changes the model learning to emphasize the
    information important for the downstream task.This
    enables us to achieve the best of both worlds:a
    prediction model trained in the original prediction
    space while also being valuable for the desired
    downstream task.We validate our approach through
    experiments conducted in two main settings: 1)
    decision-focused model learning scenarios involving
    portfolio optimization and budget allocation, and2)
    reinforcement learning in noisy environments with
    distracting states.
  }
}

@misc{zharmagambetov2023landscape,
  title = {Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information},
  author = {Arman Zharmagambetov and Brandon Amos and Aaron Ferber and Taoan Huang and Bistra Dilkina and Yuandong Tian},
  year = {2023},
  url={https://arxiv.org/abs/2307.08964},
  _venue={NeurIPS},
  abstract={
    Recent works in learning-integrated optimization have shown promise in
    settings where the optimization problem is only
    partially observed or where general-purpose
    optimizers perform poorly without expert tuning. By
    learning an optimizer g to tackle these challenging
    problems with f as the objective, the optimization
    process can be substantially accelerated by
    leveraging past experience. Training the optimizer
    can be done with supervision from known optimal
    solutions (not always available) or implicitly by
    optimizing the compound function f ∘ g , but the
    implicit approach is slow and challenging due to
    frequent calls to the optimizer and sparse
    gradients, particularly for combinatorial
    solvers. To address these challenges, we propose
    using a smooth and learnable **Landscape Surrogate**
    M instead of f ∘ g . This surrogate can be computed
    faster than g , provides dense and smooth gradients
    during training, can generalize to unseen
    optimization problems, and is efficiently learned
    via alternating optimization. We test our approach
    on both synthetic problems and real-world problems,
    achieving comparable or superior objective values
    compared to state-of-the-art baselines while
    reducing the number of calls to g . Notably, our
    approach outperforms existing methods for
    computationally expensive high-dimensional problems.
  }
}


@misc{retchin2023koopman,
  title = {Koopman Constrained Policy Optimization: A Koopman operator theoretic method for differentiable optimal control in robotics},
  author = {Matthew Retchin and Brandon Amos and Steven Brunton and Shuran Song},
  year = {2023},
  _venue={ICML Differentiable Almost Everything Workshop},
  url={https://differentiable.xyz/papers/paper_45.pdf},
  abstract={
    We introduce Koopman Constrained Policy Optimization (KCPO),
    combining implicitly differentiable model predictive
    control with a deep Koopman autoencoder for robot
    learning in unknown and nonlinear dynamical
    systems. KCPO is a new policy optimization algorithm
    that trains neural policies end-to-end with hard box
    constraints on controls. Guaranteed satisfaction of
    hard constraints helps ensure the performance and
    safety of robots. We perform imitation learning with
    KCPO to recover expert policies on the Simple
    Pendulum, Cartpole Swing-Up, Reacher, and
    Differential Drive environments, outperforming
    baseline methods in generalizing to
    out-of-distribution constraints in most environments
    after training.
  }
}

@misc{pooladian2023neural,
  title = {Neural Optimal Transport with Lagrangian Costs},
  author = {Aram-Alexandre Pooladian and Carles Domingo-Enrich and Ricky T. Q. Chen and Brandon Amos},
  year = {2023},
  _venue={ICML New Frontiers in Learning, Control, and Dynamical Systems Workshop},
  url={https://openreview.net/forum?id=myb0FKB8C9},
  abstract={
Computational efforts in optimal transport traditionally
revolve around the squared-Euclidean cost. In this
work, we choose to investigate the optimal transport
problem between probability measures when the
underlying metric space is non-Euclidean, or when
the cost function is understood to satisfy a least
action principle,also known as a Lagrangian
cost. These two generalizations are useful when
connecting observations from a physical system,
where the transport dynamics are influenced by the
geometry of the system, such as obstacles, and
allows practitioners to incorporate a priori
knowledge of the underlying system. Examples include
barriers for transport, or enforcing a certain
geometry, i.e., paths must be circular. We
demonstrate the effectiveness of this formulation on
existing synthetic examples in the literature, where
we solve the optimal transport problems in the
absence of regularization, which is novel in the
literature. Our contributions are of computational
interest, where we demonstrate the ability to
efficiently compute geodesics and amortize
spline-based paths. We demonstrate the effectiveness
of this formulation on existing synthetic examples
in the literature, where we solve the optimal
transport problems in the absence of regularization.
  }
}



@misc{fickinger2021crossdomain,
  title={Cross-Domain Imitation Learning via Optimal Transport},
  author={Arnaud Fickinger and Samuel Cohen and Stuart Russell and Brandon Amos},
  year={2022},
  url={https://arxiv.org/abs/2110.03684},
  codeurl={https://github.com/facebookresearch/gwil},
  _venue={ICLR},
  selected={true},
  abstract={
    Cross-domain imitation learning studies how to leverage expert
    demonstrations of one agent to train an imitation
    agent with a different embodiment or
    morphology. Comparing trajectories and stationary
    distributions between the expert and imitation
    agents is challenging because they live on different
    systems that may not even have the same
    dimensionality. We propose Gromov-Wasserstein
    Imitation Learning (GWIL), a method for cross-domain
    imitation that uses the Gromov-Wasserstein distance
    to align and compare states between the different
    spaces of the agents. Our theory formally
    characterizes the scenarios where GWIL preserves
    optimality, revealing its possibilities and
    limitations. We demonstrate the effectiveness of
    GWIL in non-trivial continuous control domains
    ranging from simple rigid transformation of the
    expert domain to arbitrary transformation of the
    state-action space.
  }
}

@misc{benhamu2022matching,
  title = {Matching Normalizing Flows and Probability Paths on Manifolds},
  author = {Ben-Hamu*, Heli and Cohen*, Samuel and Bose, Joey and Amos, Brandon and Grover, Aditya and Nickel, Maximilian and Chen, Ricky T. Q. and Lipman, Yaron},
  year = {2022},
  url = {https://arxiv.org/abs/2207.04711},
  _venue={ICML},
  selected={false},
  abstract={
    Continuous Normalizing Flows (CNFs) are a class of generative models
    that transform a prior distribution to a model
    distribution by solving an ordinary differential
    equation (ODE). We propose to train CNFs on
    manifolds by minimizing probability path divergence
    (PPD), a novel family of divergences between the
    probability density path generated by the CNF and a
    target probability density path. PPD is formulated
    using a logarithmic mass conservation formula which
    is a linear first order partial differential
    equation relating the log target probabilities and
    the CNF's defining vector field. PPD has several key
    benefits over existing methods: it sidesteps the
    need to solve an ODE per iteration, readily applies
    to manifold data, scales to high dimensions, and is
    compatible with a large family of target paths
    interpolating pure noise and data in finite
    time. Theoretically, PPD is shown to bound classical
    probability divergences. Empirically, we show that
    CNFs learned by minimizing PPD achieve
    state-of-the-art results in likelihoods and sample
    quality on existing low-dimensional manifold
    benchmarks, and is the first example of a generative
    model to scale to moderately high dimensional
    manifolds.
  }
}


@article{chen2022semi,
  title={Semi-Discrete Normalizing Flows through Differentiable Tessellation},
  author={Ricky T. Q. Chen and Brandon Amos and Maximilian Nickel},
  journal={arXiv preprint arXiv:2203.06832},
  year={2022},
  url={https://arxiv.org/abs/2203.06832},
  _venue={NeurIPS},
  abstract={
    Mapping between discrete and continuous distributions is a difficult
    task and many have had to resort to approximate or
    heuristical approaches. We propose a
    tessellation-based approach that directly learns
    quantization boundaries on a continuous space,
    complete with exact likelihood evaluations. This is
    done through constructing normalizing flows on
    convex polytopes parameterized through a
    differentiable Voronoi tessellation. Using a simple
    homeomorphism with an efficient log determinant
    Jacobian, we can then cheaply parameterize
    distributions on convex polytopes.

    We explore this approach in two application settings, mapping from
    discrete to continuous and vice versa. Firstly, a
    Voronoi dequantization allows automatically learning
    quantization boundaries in a multidimensional
    space. The location of boundaries and distances
    between regions can encode useful structural
    relations between the quantized discrete
    values. Secondly, a Voronoi mixture model has
    constant computation cost for likelihood evaluation
    regardless of the number of mixture
    components. Empirically, we show improvements over
    existing methods across a range of structured data
    modalities, and find that we can achieve a
    significant gain from just adding Voronoi mixtures
    to a baseline model.
  }
}

@misc{pineda2022theseus,
  url = {https://arxiv.org/abs/2207.09442},
  author = {Pineda, Luis and Fan, Taosha and Monge, Maurizio and Venkataraman, Shobha and Sodhi, Paloma and Chen, Ricky and Ortiz, Joseph and DeTone, Daniel and Wang, Austin and Anderson, Stuart and Dong, Jing and Amos, Brandon and Mukadam, Mustafa},
  title = {Theseus: A Library for Differentiable Nonlinear Optimization},
  _venue = {NeurIPS},
  codeurl={https://github.com/facebookresearch/theseus},
  year = 2022,
  selected={true},
  abstract={
    We present Theseus, an efficient application-agnostic open source
    library for differentiable nonlinear least squares
    (DNLS) optimization built on PyTorch, providing a
    common framework for end-to-end structured learning
    in robotics and vision. Existing DNLS
    implementations are application specific and do not
    always incorporate many ingredients important for
    efficiency. Theseus is application-agnostic, as we
    illustrate with several example applications that
    are built using the same underlying differentiable
    components, such as second-order optimizers,
    standard costs functions, and Lie groups. For
    efficiency, Theseus incorporates support for sparse
    solvers, automatic vectorization, batching, GPU
    acceleration, and gradient computation with implicit
    differentiation and direct loss minimization. We do
    extensive performance evaluation in a set of
    applications, demonstrating significant efficiency
    gains and better scalability when these features are
    incorporated.
  }
}

@misc{vinitsky2022nocturne,
  title = {Nocturne: a driving benchmark for multi-agent learning},
  author = {Vinitsky, Eugene and Lichtlé, Nathan and Yang, Xiaomeng and Amos, Brandon and Foerster, Jakob},
  year = {2022},
  url = {https://arxiv.org/abs/2206.09889},
  _venue={NeurIPS Datasets and Benchmarks Track},
  codeurl={https://github.com/facebookresearch/nocturne},
  abstract={
    We introduce Nocturne, a new 2D driving simulator for
    investigating multi-agent coordination under partial
    observability. The focus of Nocturne is to enable
    research into inference and theory of mind in
    real-world multi-agent settings without the
    computational overhead of computer vision and
    feature extraction from images. Agents in this
    simulator only observe an obstructed view of the
    scene, mimicking human visual sensing
    constraints. Unlike existing benchmarks that are
    bottlenecked by rendering human-like observations
    directly using a camera input, Nocturne uses
    efficient intersection methods to compute a
    vectorized set of visible features in a C++
    back-end, allowing the simulator to run at 2000+
    steps-per-second. Using open-source trajectory and
    map data, we construct a simulator to load and
    replay arbitrary trajectories and scenes from
    real-world driving data. Using this environment, we
    benchmark reinforcement-learning and
    imitation-learning agents and demonstrate that the
    agents are quite far from human-level coordination
    ability and deviate significantly from the expert
    trajectories.
  }
}

@misc{amos2021modelbased,
      title={On the model-based stochastic value gradient for continuous reinforcement learning},
      author={Brandon Amos and Samuel Stanton and Denis Yarats and Andrew Gordon Wilson},
      year={2021},
      _venue={L4DC},
      _note={Oral},
      url={https://arxiv.org/abs/2008.12775},
      codeurl={https://github.com/facebookresearch/svg},
      slidesurl={http://bamos.github.io/data/slides/2021.svg.pdf},
      _talkurl={https://youtu.be/ABS40GW7Ekk?t=5393},
      selected={true},
      abstract={
        Model-based reinforcement learning approaches add explicit domain
        knowledge to agents in hopes of improving the
        sample-efficiency in comparison to model-free
        agents. However, in practice model-based methods are
        unable to achieve the same asymptotic performance on
        challenging continuous control tasks due to the
        complexity of learning and controlling an explicit
        world model. In this paper we investigate the
        stochastic value gradient (SVG), which is a
        well-known family of methods for controlling
        continuous systems which includes model-based
        approaches that distill a model-based value
        expansion into a model-free policy. We consider a
        variant of the model-based SVG that scales to larger
        systems and uses 1) an entropy regularization to
        help with exploration, 2) a learned deterministic
        world model to improve the short-horizon value
        estimate, and 3) a learned model-free value estimate
        after the model's rollout. This SVG variation
        captures the model-free soft actor-critic method as
        an instance when the model rollout horizon is zero,
        and otherwise uses short-horizon model rollouts to
        improve the value estimate for the policy update. We
        surpass the asymptotic performance of other
        model-based methods on the proprioceptive MuJoCo
        locomotion tasks from the OpenAI gym, including a
        humanoid. We notably achieve these results with a
        simple deterministic world model without requiring
        an ensemble.
      }
}

@inproceedings{cohen2021riemannian,
  title={{Riemannian Convex Potential Maps}},
  author={Cohen*, Samuel and Amos*, Brandon and Lipman, Yaron},
  booktitle={ICML},
  _venue={ICML},
  year={2021},
  url={https://arxiv.org/abs/2106.10272},
  codeurl={https://github.com/facebookresearch/rcpm},
  slidesurl={http://bamos.github.io/data/slides/2021.rcpm.pdf},
  selected={true},
  abstract={
    Modeling distributions on Riemannian manifolds is a crucial
    component in understanding non-Euclidean data that
    arises, e.g., in physics and geology. The budding
    approaches in this space are limited by
    representational and computational tradeoffs. We
    propose and study a class of flows that uses convex
    potentials from Riemannian optimal transport. These
    are universal and can model distributions on any
    compact Riemannian manifold without requiring domain
    knowledge of the manifold to be integrated into the
    architecture. We demonstrate that these flows can
    model standard distributions on spheres, and tori,
    on synthetic and geological data.
  }
}

@@inproceedings{paulus2021comboptnet,
  title={CombOptNet: Fit the Right NP-Hard Problem by Learning Integer Programming Constraints},
  author={Paulus, Anselm and Rol{\'\i}nek, Michal and Musil, V{\'\i}t and Amos, Brandon and Martius, Georg},
  booktitle={ICML},
  _venue={ICML},
  year={2021},
  url={https://arxiv.org/abs/2105.02343},
  codeurl={https://github.com/martius-lab/CombOptNet},
  abstract={
    Bridging logical and algorithmic reasoning with modern machine
    learning techniques is a fundamental challenge with
    potentially transformative impact. On the
    algorithmic side, many NP-hard problems can be
    expressed as integer programs, in which the
    constraints play the role of their "combinatorial
    specification". In this work, we aim to integrate
    integer programming solvers into neural network
    architectures as layers capable of learning both the
    cost terms and the constraints. The resulting
    end-to-end trainable architectures jointly extract
    features from raw data and solve a suitable
    (learned) combinatorial problem with
    state-of-the-art integer programming solvers. We
    demonstrate the potential of such layers with an
    extensive performance analysis on synthetic data and
    with a demonstration on a competitive computer
    vision keypoint matching benchmark.
  }
}

@inproceedings{fickinger2021scalable,
  year={2021},
  booktitle={NeurIPS},
  _venue={NeurIPS},
  url={https://arxiv.org/abs/2109.15316},
  title={{Scalable Online Planning via Reinforcement Learning Fine-Tuning}},
  author={Arnaud Fickinger and Hengyuan Hu and Brandon Amos and Stuart Russell and Noam Brown},
  year={2021},
  abstract={
    Lookahead search has been a critical component of recent AI successes,
    such as in the games of chess, go, and
    poker. However, the search methods used in these
    games, and in many other settings, are
    tabular. Tabular search methods do not scale well
    with the size of the search space, and this problem
    is exacerbated by stochasticity and partial
    observability. In this work we replace tabular
    search with online model-based fine-tuning of a
    policy neural network via reinforcement learning,
    and show that this approach outperforms
    state-of-the-art search algorithms in benchmark
    settings. In particular, we use our search algorithm
    to achieve a new state-of-the-art result in
    self-play Hanabi, and show the generality of our
    algorithm by also showing that it outperforms
    tabular search in the Atari game Ms. Pacman.
  }
}

@inproceedings{cohen2020aligning,
      title={Aligning Time Series on Incomparable Spaces},
      author={Samuel Cohen and Giulia Luise and Alexander Terenin and Brandon Amos and Marc Peter Deisenroth},
      booktitle={AISTATS},
      year={2021},
      _venue={AISTATS},
      url={https://arxiv.org/abs/2006.12648},
      codeurl={https://github.com/samcohen16/Aligning-Time-Series},
      slidesurl={http://bamos.github.io/data/slides/2021.gdtw.pdf},
      abstract={
        Dynamic time warping (DTW) is a useful method for aligning, comparing
        and combining time series, but it requires them to
        live in comparable spaces. In this work, we consider
        a setting in which time series live on different
        spaces without a sensible ground metric, causing DTW
        to become ill-defined. To alleviate this, we propose
        Gromov dynamic time warping (GDTW), a distance
        between time series on potentially incomparable
        spaces that avoids the comparability requirement by
        instead considering intra-relational geometry. We
        derive a Frank-Wolfe algorithm for computing it and
        demonstrate its effectiveness at aligning, combining
        and comparing time series living on incomparable
        spaces. We further propose a smoothed version of
        GDTW as a differentiable loss and assess its
        properties in a variety of settings, including
        barycentric averaging, generative modeling and
        imitation learning.
      },
}

@inproceedings{chen2021learning,
      title={Learning Neural Event Functions for Ordinary Differential Equations},
      author={Ricky T. Q. Chen and Brandon Amos and Maximilian Nickel},
      booktitle={ICLR},
      _venue={ICLR},
      year={2021},
      url={https://arxiv.org/abs/2011.03902},
      codeurl={https://github.com/rtqichen/torchdiffeq},
      abstract={
        The existing Neural ODE formulation relies on an explicit
        knowledge of the termination time. We extend Neural
        ODEs to implicitly defined termination criteria
        modeled by neural event functions, which can be
        chained together and differentiated through. Neural
        Event ODEs are capable of modeling discrete
        (instantaneous) changes in a continuous-time system,
        without prior knowledge of when these changes should
        occur or how many such changes should exist. We test
        our approach in modeling hybrid discrete- and
        continuous- systems such as switching dynamical
        systems and collision in multi-body systems, and we
        propose simulation-based training of point processes
        with applications in discrete control.
      }
}

@inproceedings{chen2021neural,
      title={Neural Spatio-Temporal Point Processes},
      author={Ricky T. Q. Chen and Brandon Amos and Maximilian Nickel},
      booktitle={ICLR},
      _venue={ICLR},
      year={2021},
      url={https://arxiv.org/abs/2011.04583},
      codeurl={https://github.com/facebookresearch/neural_stpp},
      abstract={
        We propose a new class of parameterizations for spatio-temporal
        point processes which leverage Neural ODEs as a
        computational method and enable flexible,
        high-fidelity models of discrete events that are
        localized in continuous time and space. Central to
        our approach is a combination of recurrent
        continuous-time neural networks with two novel
        neural architectures, i.e., Jump and Attentive
        Continuous-time Normalizing Flows. This approach
        allows us to learn complex distributions for both
        the spatial and temporal domain and to condition
        non-trivially on the observed event history. We
        validate our models on data sets from a wide variety
        of contexts such as seismology, epidemiology, urban
        mobility, and neuroscience.
      }
}

@inproceedings{yarats2021improving,
  title={{Improving Sample Efficiency in Model-Free Reinforcement Learning from Images}},
  author={Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
  journal={arXiv preprint arXiv:1910.01741},
  booktitle={AAAI},
  _venue={AAAI},
  year=2021,
  url={https://arxiv.org/abs/1910.01741},
  codeurl={https://sites.google.com/view/sac-ae},
  abstract={
    Training an agent to solve control tasks directly from
    high-dimensional images with model-free
    reinforcement learning (RL) has proven
    difficult. The agent needs to learn a latent
    representation together with a control policy to
    perform the task. Fitting a high-capacity encoder
    using a scarce reward signal is not only sample
    inefficient, but also prone to suboptimal
    convergence. Two ways to improve sample efficiency
    are to extract relevant features for the task and
    use off-policy algorithms. We dissect various
    approaches of learning good latent features, and
    conclude that the image reconstruction loss is the
    essential ingredient that enables efficient and
    stable representation learning in image-based
    RL. Following these findings, we devise an
    off-policy actor-critic algorithm with an auxiliary
    decoder that trains end-to-end and matches
    state-of-the-art performance across both model-free
    and model-based algorithms on many challenging
    control tasks. We release our code to encourage
    future research on image-based RL.
  }
}

@article{venkataraman2021neural,
  title={Neural Fixed-Point Acceleration for Convex Optimization},
  author={Shobha Venkataraman* and Brandon Amos*},
  year={2021},
  url={https://arxiv.org/abs/2107.10254},
  _venue={ICML AutoML Workshop},
  codeurl={https://github.com/facebookresearch/neural-scs},
  abstract={
    Fixed-point iterations are at the heart of numerical computing and
    are often a computational bottleneck in real-time
    applications that typically need a fast solution of
    moderate accuracy. We present neural fixed-point
    acceleration which combines ideas from meta-learning
    and classical acceleration methods to automatically
    learn to accelerate fixed-point problems that are
    drawn from a distribution. We apply our framework to
    SCS, the state-of-the-art solver for convex cone
    programming, and design models and loss functions to
    overcome the challenges of learning over unrolled
    optimization and acceleration instabilities. Our
    work brings neural acceleration into any
    optimization problem expressible with CVXPY.
  }
}

@misc{cohen2021sliced,
    title={Sliced Multi-Marginal Optimal Transport},
    author={Samuel Cohen and Alexander Terenin and Yannik Pitcan and Brandon Amos and Marc Peter Deisenroth and K S Sesh Kumar},
    year={2021},
    url={https://arxiv.org/abs/2102.07115},
    _venue={NeurIPS OTML Workshop},
    abstract={
        Multi-marginal optimal transport enables one to compare multiple
        probability measures, which increasingly finds
        application in multi-task learning problems. One
        practical limitation of multi-marginal transport is
        computational scalability in the number of measures,
        samples and dimensionality. In this work, we propose
        a multi-marginal optimal transport paradigm based on
        random one-dimensional projections, whose
        (generalized) distance we term the sliced
        multi-marginal Wasserstein distance. To construct
        this distance, we introduce a characterization of
        the one-dimensional multi-marginal Kantorovich
        problem and use it to highlight a number of
        properties of the sliced multi-marginal Wasserstein
        distance. In particular, we show that (i) the sliced
        multi-marginal Wasserstein distance is a
        (generalized) metric that induces the same topology
        as the standard Wasserstein distance, (ii) it admits
        a dimension-free sample complexity, (iii) it is
        tightly connected with the problem of barycentric
        averaging under the sliced-Wasserstein metric. We
        conclude by illustrating the sliced multi-marginal
        Wasserstein on multi-task density estimation and
        multi-dynamics reinforcement learning problems.
    }
}

@misc{richterpowell2021input,
    title={Input Convex Gradient Networks},
    author={Jack Richter-Powell and Jonathan Lorraine and Brandon Amos},
    year={2021},
    _venue={NeurIPS OTML Workshop},
    url={https://arxiv.org/abs/2111.12187},
    abstract={
    The gradients of convex functions are expressive models of non-trivial
    vector fields. For example, Brenier's theorem yields
    that the optimal transport map between any two
    measures on Euclidean space under the squared
    distance is realized as a convex gradient, which is
    a key insight used in recent generative flow
    models. In this paper, we study how to model convex
    gradients by integrating a Jacobian-vector product
    parameterized by a neural network, which we call the
    Input Convex Gradient Network (ICGN). We
    theoretically study ICGNs and compare them to taking
    the gradient of an Input-Convex Neural Network
    (ICNN), empirically demonstrating that a single
    layer ICGN can fit a toy example better than a
    single layer ICNN. Lastly, we explore extensions to
    deeper networks and connections to constructions
    from Riemannian geometry.
    }
}

@inproceedings{cohen2021imitation,
  title={Imitation Learning from Pixel Observations for Continuous Control},
  author={Cohen, Samuel and Amos, Brandon and Deisenroth, Marc Peter and Henaff, Mikael and Vinitsky, Eugene and Yarats, Denis},
  _venue={NeurIPS DeepRL Workshop},
  year={2021},
  url={https://openreview.net/pdf?id=Xe5MFhFvYGX},
  abstract={
We study imitation learning from visual observations only for
controlling dynamical systems with continuous states
and actions. This setting is attractive due to the
large amount of video data available from which
agents could learn from. However, it is challenging
due to i) not observing the actions and ii) the
high-dimensional visual space. In this setting, we
explore recipes for imitation learning based on
adversarial learning and optimal transport. These
recipes enable us to scale these methods to attain
expert-level performance on visual continuous
control tasks in the DeepMind control suite. We
investigate the tradeoffs of these approaches and
present a comprehensive evaluation of the key design
choices. To encourage reproducible research in this
area, we provide an easy-to-use implementation for
benchmarking visual imitation learning, including
our methods and expert demonstrations.
}
}

@article{pineda2021mbrl,
  title={MBRL-Lib: A Modular Library for Model-based Reinforcement Learning},
  author={Pineda, Luis and Amos, Brandon and Zhang, Amy and Lambert, Nathan and Calandra, Roberto},
  year={2021},
  url={https://arxiv.org/abs/2104.10159},
  _venue={arXiv},
  codeurl={https://github.com/facebookresearch/mbrl-lib},
  abstract={
    Model-based reinforcement learning is a compelling framework for
    data-efficient learning of agents that interact with
    the world. This family of algorithms has many
    subcomponents that need to be carefully selected and
    tuned. As a result the entry-bar for researchers to
    approach the field and to deploy it in real-world
    tasks can be daunting. In this paper, we present
    MBRL-Lib -- a machine learning library for
    model-based reinforcement learning in continuous
    state-action spaces based on PyTorch. MBRL-Lib is
    designed as a platform for both researchers, to
    easily develop, debug and compare new algorithms,
    and non-expert user, to lower the entry-bar of
    deploying state-of-the-art algorithms.
  }
}



@inproceedings{amos2020differentiable,
  title={{The Differentiable Cross-Entropy Method}},
  author={Amos, Brandon and Yarats, Denis},
  booktitle={ICML},
  _venue={ICML},
  year={2020},
  url={https://arxiv.org/abs/1909.12830},
  codeurl={https://github.com/facebookresearch/dcem},
  slidesurl={http://bamos.github.io/data/slides/2020.dcem.pdf},
  selected={true},
  abstract={
    We study the Cross-Entropy Method (CEM) for the non-convex
    optimization of a continuous and parameterized
    objective function and introduce a differentiable
    variant (DCEM) that enables us to differentiate the
    output of CEM with respect to the objective
    function's parameters. In the machine learning
    setting this brings CEM inside of the end-to-end
    learning pipeline where this has otherwise been
    impossible. We show applications in a synthetic
    energy-based structured prediction task and in
    non-convex continuous control. In the control
    setting we show on the simulated cheetah and walker
    tasks that we can embed their optimal action
    sequences with DCEM and then use policy optimization
    to fine-tune components of the controller as a step
    towards combining model-based and model-free RL.
  }
}


@inproceedings{lambert2020objective,
  title={Objective Mismatch in Model-based Reinforcement Learning},
  author={Lambert, Nathan and Amos, Brandon and Yadan, Omry and Calandra, Roberto},
  year={2020},
  booktitle={L4DC},
  _venue={L4DC},
  year={2020},
  url={https://arxiv.org/abs/2002.04523},
  abstract={
  Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, with little development of the general framework. In this paper, we identify a fundamental issue of the standard MBRL framework--what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model with respect to the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of one-step ahead predictions is not always correlated with control performance. This observation highlights a critical limitation in the MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of research for addressing this issue.
  }
}

@article{amos2020QNSTOP,
  title={{{QNSTOP: Quasi-Newton Algorithm for Stochastic Optimization}}},
  author={Brandon Amos and David Easterling and Layne T. Watson and
    William Thacker and Brent Castle and Michael Trosset},
  journal={},
  _venue={ACM TOMS},
  year={2020},
  keywords={journal},
  url={https://vtechworks.lib.vt.edu/bitstream/handle/10919/49672/qnTOMS14.pdf},
  codeurl={https://github.com/vtopt/qnstop},
  abstract={
    QNSTOP consists of serial and parallel (OpenMP) Fortran 2003 codes for the
    quasi-Newton stochastic optimization method of Castle and Trosset. For
    stochastic problems, convergence theory exists for the particular
    algorithmic choices and parameter values used in QNSTOP. Both the parallel
    driver subroutine, which offers several parallel decomposition strategies,
    and the serial driver subroutine can be used for stochastic optimization or
    deterministic global optimization, based on an input switch. QNSTOP is
    particularly effective for “noisy” deterministic problems, using only
    objective function values. Some performance data for computational systems
    biology problems is given.
  }
}

@article{sercu2020neural,
  title={Neural Potts Model},
  author={Sercu, Tom and Verkuil, Robert and Meier, Joshua and Amos, Brandon and Lin, Zeming and Chen, Caroline and Liu, Jason and LeCun, Yann and Rives, Alexander},
  year={2020},
  _venue = {MLCB},
  url={https://www.biorxiv.org/content/10.1101/2021.04.08.439084v1.abstract},
  abstract = {
    We propose the Neural Potts Model objective as an amortized
    optimization problem. The objective enables training
    a single model with shared parameters to explicitly
    model energy landscapes across multiple protein
    families. Given a protein sequence as input, the
    model is trained to predict a pairwise coupling
    matrix for a Potts model energy function describing
    the local evolutionary landscape of the
    sequence. Couplings can be predicted for novel
    sequences. A controlled ablation experiment
    assessing unsupervised contact prediction on sets of
    related protein families finds a gain from
    amortization for low-depth multiple sequence
    alignments; the result is then confirmed on a
    database with broad coverage of protein sequences.
  }
}

@article{lou2020riemannian,
  title={Deep Riemannian Manifold Learning},
  author={Lou, Aaron and Nickel, Maximilian and Amos, Brandon},
  year={2020},
  _venue = {NeurIPS Geo4dl Workshop},
  url={https://drive.google.com/file/d/1Ewro0Ne1tvK15nHyYopY4wZ59QTVB-1c/view},
  abstract = {
We present a new class of learnable Riemannian manifolds with a metric
parameterized by a deep neural network. The core manifold operations–specifically
the Riemannian exponential and logarithmic maps–are solved using approximate
numerical techniques. Input and parameter gradients are computed with an
adjoint sensitivity analysis. This enables us to fit geodesics and distances with
gradient-based optimization of both on-manifold values and the manifold itself.
We demonstrate our method’s capability to model smooth, flexible metric structures
in graph embedding tasks.
  }
}


@phdthesis{amos2019differentiable,
  author       = {Brandon Amos},
  title        = {{Differentiable Optimization-Based Modeling for Machine Learning}},
  school       = {Carnegie Mellon University},
  year         = 2019,
  _venue = {Ph.D. Thesis},
  codeurl={https://github.com/bamos/thesis},
  url={https://github.com/bamos/thesis/raw/master/bamos_thesis.pdf},
  selected={true},
  abstract={
Domain-specific modeling priors and specialized components are becoming
increasingly important to the machine learning field. These components integrate specialized knowledge that we have as humans into model. We argue in
this thesis that optimization methods provide an expressive set of operations
that should be part of the machine learning practitioner’s modeling toolbox.
We present two foundational approaches for optimization-based modeling:
1) the OptNet architecture that integrates optimization problems as individual
layers in larger end-to-end trainable deep networks, and 2) the input-convex
neural network (ICNN) architecture that helps make inference and learning in
deep energy-based models and structured prediction more tractable.
We then show how to use the OptNet approach 1) as a way of combining
model-free and model-based reinforcement learning and 2) for top-k learning
problems. We conclude by showing how to differentiate cone programs and turn
the cvxpy domain specific language into a differentiable optimization layer that
enables rapid prototyping of the approaches in this thesis.
  }
}


@inproceedings{amos2019differentiable3,
  title={{Differentiable Convex Optimization Layers}},
  author={Agrawal*, Akshay and Amos*, Brandon and Barratt*, Shane and Boyd*, Stephen and Diamond*, Steven and Kolter*, J. Zico},
  year={2019},
  booktitle={NeurIPS},
  _venue={NeurIPS},
  url={http://web.stanford.edu/~boyd/papers/pdf/diff_cvxpy.pdf},
  codeurl={https://github.com/cvxgrp/cvxpylayers},
  selected={true},
  abstract={
    Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver’s solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.
  }
}

@inproceedings{amos2018learning,
  title={Learning Awareness Models},
  author={Brandon Amos and Laurent Dinh and Serkan Cabi and Thomas Roth{\"o}rl and Sergio G{\'o}mez Colmenarejo and Alistair Muldal and Tom Erez and Yuval Tassa and Nando de Freitas and Misha Denil},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://arxiv.org/abs/1804.06318},
  _venue={ICLR},
  selected={true},
  abstract={
    We consider the setting of an agent with a fixed body interacting with an
    unknown and uncertain external world. We show that models
    trained to predict proprioceptive information about the
    agent's body come to represent objects in the external world.
    In spite of being trained with only internally available
    signals, these dynamic body models come to represent external
    objects through the necessity of predicting their effects on
    the agent's own body. That is, the model learns holistic
    persistent representations of objects in the world, even
    though the only training signals are body signals. Our
    dynamics model is able to successfully predict distributions
    over 132 sensor readings over 100 steps into the future and we
    demonstrate that even when the body is no longer in contact
    with an object, the latent variables of the dynamics model
    continue to represent its shape. We show that active data
    collection by maximizing the entropy of predictions about the
    body---touch sensors, proprioception and vestibular
    information---leads to learning of dynamic models that show
    superior performance when used for control. We also collect
    data from a real robotic hand and show that the same models
    can be used to answer questions about properties of objects in
    the real world. Videos with qualitative results of our models
    are available <a href="https://goo.gl/mZuqAV">here</a>.
  }
}

@inproceedings{amos2018end,
  title={{{Differentiable MPC for End-to-end Planning and Control}}},
  author={Amos, Brandon and Rodriguez, Ivan Dario Jimenez and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
  year={2018},
  booktitle={NeurIPS},
  _venue={NeurIPS},
  url={https://arxiv.org/abs/1810.13400},
  codeurl={https://locuslab.github.io/mpc.pytorch/},
  selected={true},
  abstract={
  In this paper we present foundations for using model predictive control (MPC) as a differentiable policy class in reinforcement learning. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the solver. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning in a larger system. We empirically show results in an imitation learning setting, demonstrating that we can recover the underlying dynamics and cost more efficiently and reliably than with a generic neural network policy class
  }
}

@inproceedings{amos2017optnet,
  title = "{OptNet: Differentiable Optimization as a Layer in Neural Networks}",
  author={Brandon Amos and J. Zico Kolter},
  booktitle={ICML},
  _venue={ICML},
  year={2017},
  url={http://arxiv.org/abs/1703.00443},
  codeurl={https://github.com/locuslab/optnet},
  slidesurl={http://bamos.github.io/data/slides/2017.optnet.pdf},
  _talkurl={https://vimeo.com/238242080},
  selected={true},
  abstract={
    This paper presents OptNet, a network architecture that integrates
    optimization problems (here, specifically in the form of quadratic programs)
    as individual layers in larger end-to-end trainable deep networks.
    These layers encode constraints and complex dependencies
    between the hidden states that traditional convolutional and
    fully-connected layers often cannot capture.
    In this paper, we explore the foundations for such an architecture:
    we show how techniques from sensitivity analysis, bilevel
    optimization, and implicit differentiation can be used to
    exactly differentiate through these layers and with respect
    to layer parameters;
    we develop a highly efficient solver for these layers that exploits fast
    GPU-based batch solves within a primal-dual interior point method, and which
    provides backpropagation gradients with virtually no additional cost on top of
    the solve;
    and we highlight the application of these approaches in several problems.
    In one notable example, we show that the method is
    capable of learning to play mini-Sudoku (4x4) given just input and output games,
    with no a priori information about the rules of the game;
    this highlights the ability of our architecture to learn hard
    constraints better than other neural architectures.
  }
}


@inproceedings{amos2017input,
  title={Input Convex Neural Networks},
  author={Brandon Amos and Lei Xu and J. Zico Kolter},
  booktitle={ICML},
  _venue={ICML},
  codeurl={https://github.com/locuslab/icnn},
  year={2017},
  url={http://arxiv.org/abs/1609.07152},
  slidesurl={http://bamos.github.io/data/slides/2017.icnn.pdf},
  _talkurl={https://vimeo.com/238242121},
  selected={true},
  abstract={
    This paper presents the input convex neural network
    architecture. These are scalar-valued (potentially deep) neural
    networks with constraints on the network parameters such that the
    output of the network is a convex function of (some of) the inputs.
    The networks allow for efficient inference via optimization over some
    inputs to the network given others, and can be applied to settings
    including structured prediction, data imputation, reinforcement
    learning, and others. In this paper we lay the basic groundwork for
    these models, proposing methods for inference, optimization and
    learning, and analyze their representational power. We show that many
    existing neural network architectures can be made input-convex with
    a minor modification, and develop specialized optimization
    algorithms tailored to this setting. Finally, we highlight the
    performance of the methods on multi-label prediction, image
    completion, and reinforcement learning problems, where we show
    improvement over the existing state of the art in many cases.
  }
}

@inproceedings{donti2017task,
  title={Task-based End-to-end Model Learning},
  author={Donti, Priya L. and Amos, Brandon and Kolter, J. Zico},
  year={2017},
  booktitle={NeurIPS},
  _venue={NeurIPS},
  codeurl={https://github.com/locuslab/e2e-model-learning},
  url={http://arxiv.org/abs/1703.04529},
  abstract={
    As machine learning techniques have become more ubiquitous, it has
    become common to see machine learning prediction algorithms operating
    within some larger process. However, the criteria by which we train
    machine learning algorithms often differ from the ultimate criteria on
    which we evaluate them. This paper proposes an end-to-end approach for
    learning probabilistic machine learning models within the context of
    stochastic programming, in a manner that directly captures the
    ultimate task-based objective for which they will be used. We then
    present two experimental evaluations of the proposed approach, one as
    applied to a generic inventory stock problem and the second to a
    real-world electrical grid scheduling task. In both cases, we show
    that the proposed approach can outperform both a traditional modeling
    approach and a purely black-box policy optimization approach.
  }
}

@techreport{amos2016openface,
  title={OpenFace: A general-purpose face recognition
    library with mobile applications},
  author={Amos, Brandon and Bartosz Ludwiczuk and Satyanarayanan, Mahadev},
  _venue={CMU},
  year={2016},
  institution={Technical Report CMU-CS-16-118, CMU School of Computer Science},
  url={http://reports-archive.adm.cs.cmu.edu/anon/anon/2016/CMU-CS-16-118.pdf},
  codeurl={https://cmusatyalab.github.io/openface},
  selected={true},
  abstract={
    Cameras are becoming ubiquitous in the Internet of Things (IoT) and
    can use face recognition technology to improve context. There is a
    large accuracy gap between today's publicly available face recognition
    systems and the state-of-the-art private face recognition
    systems. This paper presents our OpenFace face recognition library
    that bridges this accuracy gap. We show that OpenFace provides
    near-human accuracy on the LFW benchmark and present a new
    classification benchmark for mobile scenarios. This paper is intended
    for non-experts interested in using OpenFace and provides a light
    introduction to the deep neural network techniques we use.

    We released OpenFace in October 2015 as an open source library under
    the Apache 2.0 license. It is available at:
    <http://cmusatyalab.github.io/openface/>
  }
}


@inproceedings{zhao2016collapsed,
  title={{{Collapsed Variational Inference for Sum-Product Networks}}},
  author={Han Zhao and Tameem Adel and Geoff Gordon and Brandon Amos},
  booktitle={ICML},
  _venue={ICML},
  year={2016},
  url={http://proceedings.mlr.press/v48/zhaoa16.html},
  abstract={
    Sum-Product Networks (SPNs) are probabilistic inference machines that admit
    exact inference in linear time in the size of the network. Existing
    parameter learning approaches for SPNs are largely based on the maximum
    likelihood principle and hence are subject to overfitting compared to
    more Bayesian approaches. Exact Bayesian posterior inference for SPNs is
    computationally intractable. Both standard variational inference and
    posterior sampling for SPNs are computationally infeasible even for
    networks of moderate size due to the large number of local latent
    variables per instance. In this work, we propose a novel deterministic
    collapsed variational inference algorithm for SPNs that is
    computationally efficient, easy to implement and at the same time allows
    us to incorporate prior information into the optimization formulation.
    Extensive experiments show a significant improvement in accuracy compared
    with a maximum likelihood based approach.
  }
}
@article{chen2017quasi,
  title={Quasi-Newton Stochastic Optimization Algorithm for Parameter Estimation of a Stochastic Model of the Budding Yeast Cell Cycle},
  author={Chen, Minghan and Amos, Brandon and Watson, Layne T. and Tyson, John and Cao, Yang and Shaffer, Cliff and Trosset, Michael and Oguz, Cihan and Kakoti, Gisella},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  year={2017},
  url={https://par.nsf.gov/servlets/purl/10111392},
  publisher={IEEE},
  _venue={IEEE/ACM TCBB},
  abstract={
Parameter estimation in discrete or continuous deterministic cell
cycle models is challenging for several reasons,
including the nature of what can be observed, and
the accuracy and quantity of those observations. The
challenge is even greater for stochastic models,
where the number of simulations and amount of
empirical data must be even larger to obtain
statistically valid parameter estimates. The two
main contributions of this work are (1) stochastic
model parameter estimation based on directly
matching multivariate probability distributions, and
(2) a new quasi-Newton algorithm class QNSTOP for
stochastic optimization problems. QNSTOP directly
uses the random objective function value samples
rather than creating ensemble statistics. QNSTOP is
used here to directly match empirical and simulated
joint probability distributions rather than matching
summary statistics. Results are given for a current
state-of-the-art stochastic cell cycle model of
budding yeast, whose predictions match well some
summary statistics and one-dimensional distributions
from empirical data, but do not match well the
empirical joint distributions. The nature of the
mismatch provides insight into the weakness in the
stochastic model.
  }
}

@article{satyanarayanan2015edge,
  title={Edge Analytics in the Internet of Things},
  author={
    Mahadev Satyanarayanan and Pieter Simoens and Yu Xiao and
    Padmanabhan Pillai and Zhuo Chen and Kiryong Ha and
    Wenlu Hu and Brandon Amos
  },
  journal={IEEE Pervasive Computing},
  number={2},
  pages={24--31},
  _venue={IEEE Pervasive Computing},
  year={2015},
  publisher={IEEE},
  url={https://www.cs.cmu.edu/~satya/docdir/satya-edge2015.pdf},
  abstract={
    High-data-rate sensors, such as video cameras, are becoming ubiquitous in the
    Internet of Things. This article describes GigaSight, an Internet-scale
    repository of crowd-sourced video content, with strong enforcement of privacy
    preferences and access controls. The GigaSight architecture is a federated
    system of VM-based cloudlets that perform video analytics at the edge of the
    Internet, thus reducing the demand for ingress bandwidth into the cloud.
    Denaturing, which is an owner-specific reduction in fidelity of video content
    to preserve privacy, is one form of analytics on cloudlets. Content-based
    indexing for search is another form of cloudlet-based analytics. This article
    is part of a special issue on smart spaces.
  }
}

@article{turner2015bad,
  title={{{Bad Parts: Are Our Manufacturing Systems at Risk of Silent Cyberattacks?}}},
  author={Turner, Hamilton and White, Jules and Camelio, Jaime A. and Williams, Christopher and Amos, Brandon and Parker, Robert},
  journal={Security \& Privacy, IEEE},
  volume={13},
  number={3},
  pages={40--47},
  _venue={IEEE Security \& Privacy},
  year={2015},
  publisher={IEEE},
  keywords={magazine},
  url={http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7118094},
  abstract={
    Recent cyberattacks have highlighted the risk of physical equipment operating
    outside designed tolerances to produce catastrophic failures. A related
    threat is cyberattacks that change the design and manufacturing of a
    machine's part, such as an automobile brake component, so it no longer
    functions properly. These risks stem from the lack of cyber-physical models
    to identify ongoing attacks as well as the lack of rigorous application of
    known cybersecurity best practices. To protect manufacturing processes in the
    future, research will be needed on a number of critical cyber-physical
    manufacturing security topics.
  }
}
@inproceedings{ha2017you,
  title={You can teach elephants to dance: agile VM handoff for edge computing},
  author={Ha, Kiryong and Abe, Yoshihisa and Eiszler, Thomas and Chen, Zhuo and Hu, Wenlu and Amos, Brandon and Upadhyaya, Rohit and Pillai, Padmanabhan and Satyanarayanan, Mahadev},
  booktitle={Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
  pages={12},
  year={2017},
  url={https://dl.acm.org/doi/10.1145/3132211.3134453},
  organization={ACM},
  _venue={SEC},
  abstract={
VM handoff enables rapid and transparent placement changes to
executing code in edge computing use cases where the
safety and management attributes of VM encapsulation
are important. This versatile primitive offers the
functionality of classic live migration but is
highly optimized for the edge. Over WAN bandwidths
ranging from 5 to 25 Mbps, VM handoff migrates a
running 8 GB VM in about a minute, with a downtime
of a few tens of seconds. By dynamically adapting to
varying network bandwidth and processing load, VM
handoff is more than an order of magnitude faster
than live migration at those bandwidths.
  }
}

@inproceedings{chen2017empirical,
  author = {Chen, Zhuo and Hu, Wenlu and Wang, Junjue and Zhao, Siyan and Amos, Brandon and Wu, Guanhang and Ha, Kiryong and Elgazzar, Khalid and Pillai, Padmanabhan and Klatzky, Roberta and Siewiorek, Daniel and Satyanarayanan, Mahadev},
  title = {An Empirical Study of Latency in an Emerging Class of Edge Computing Applications for Wearable Cognitive Assistance},
  booktitle={Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
  url={https://www.cs.cmu.edu/~zhuoc/papers/latency2017.pdf},
  pages={12},
  year={2017},
  organization={ACM},
  _venue={SEC},
  abstract={
An emerging class of interactive wearable cognitive assistance
applications is poised to become one of the key
demonstrators of edge computing infrastructure. In
this paper, we design seven such applications and
evaluate their performance in terms of latency
across a range of edge computing configurations,
mobile hardware, and wireless networks, including 4G
LTE. We also devise a novel multi-algorithm approach
that leverages temporal locality to reduce
end-to-end latency by 60% to 70%, without
sacrificing accuracy. Finally, we derive target
latencies for our applications, and show that edge
computing is crucial to meeting these targets.
 }
}

@inproceedings{hu2016quantifying,
  title={Quantifying the impact of edge computing on mobile applications},
  author={Hu, Wenlu and Gao, Ying and Ha, Kiryong and Wang, Junjue and Amos, Brandon and Chen, Zhuo and Pillai, Padmanabhan and Satyanarayanan, Mahadev},
  booktitle={Proceedings of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems},
  url={https://dl.acm.org/doi/10.1145/2967360.2967369},
  pages={5},
  year={2016},
  organization={ACM},
  _venue={ACM SIGOPS},
  abstract={
Computational offloading services at the edge of the Internet for
mobile devices are becoming a reality. Using a wide
range of mobile applications, we explore how such
infrastructure improves latency and energy
consumption relative to the cloud. We present
experimental results from WiFi and 4G LTE networks
that confirm substantial wins from edge computing
for highly interactive mobile applications.
  }
}

@inproceedings{davies2016privacy,
  title={{Privacy mediators: helping IoT cross the chasm}},
  author={
    Davies, Nigel and Taft, Nina and
    Satyanarayanan, Mahadev and Clinch, Sarah and
    Amos, Brandon
  },
  booktitle={HotMobile},
  _venue={HotMobile},
  year={2016},
  url={http://eprints.lancs.ac.uk/78255/1/44691.pdf},
  abstract={
    Unease over data privacy will retard consumer acceptance of IoT
    deployments. The primary source of discomfort is a lack of user
    control over raw data that is streamed directly from sensors to the
    cloud. This is a direct consequence of the over-centralization of
    today’s cloud-based IoT hub designs. We propose a solution that
    interposes a locally-controlled software component called a privacy
    mediator on every raw sensor stream. Each mediator is in the same
    administrative domain as the sensors whose data is being collected,
    and dynamically enforces the current privacy policies of the owners
    of the sensors or mobile users within the domain. This solution necessitates
    a logical point of presence for mediators within the administrative
    boundaries of each organization. Such points of presence
    are provided by cloudlets, which are small locally-administered data
    centers at the edge of the Internet that can support code mobility.
    The use of cloudlet-based mediators aligns well with natural personal
    and organizational boundaries of trust and responsibility.
  }
}

@inproceedings{chen2015early,
  title={{{Early Implementation Experience with Wearable Cognitive Assistance Applications}}},
  author={Chen, Zhuo and Jiang, Lu and Hu, Wenlu and Ha, Kiryong and Amos, Brandon and Pillai, Padmanabhan and Hauptmann, Alex and Satyanarayanan, Mahadev},
  booktitle={WearSys},
  _venue={WearSys},
  year={2015},
  url={http://www.cs.cmu.edu/~satya/docdir/chen-wearsys2015.pdf},
  abstract={
    A cognitive assistance application combines a wearable device such
    as Google Glass with cloudlet processing to provide step-by-step
    guidance on a complex task. In this paper, we focus on user assistance
    for narrow and well-defined tasks that require specialized
    knowledge and/or skills. We describe proof-of-concept implementations
    for four different tasks: assembling 2D Lego models, freehand
    sketching, playing ping-pong, and recommending context-relevant
    YouTube tutorials. We then reflect on the difficulties we faced in
    building these applications, and suggest future research that could
    simplify the creation of similar applications.
  }
}

@inproceedings{hu2014case,
  title={{{The Case for Offload Shaping}}},
  author={
    Wenlu Hu and Brandon Amos and Zhuo Chen and Kiryong Ha and
    Wolfgang Richter and Padmanabhan Pillai and Benjamin Gilbert and
    Jan Harkes and Mahadev Satyanarayanan
  },
  booktitle={HotMobile},
  _venue={HotMobile},
  year={2015},
  url={http://www.cs.cmu.edu/~satya/docdir/hu-hotmobile2015.pdf},
  abstract={
    When offloading computation from a mobile device, we show
    that it can pay to perform additional on-device work in order
    to reduce the offloading workload. We call this offload shaping,
    and demonstrate its application at many different levels
    of abstraction using a variety of techniques. We show that
    offload shaping can produce significant reduction in resource
    demand, with little loss of application-level fidelity
  }
}

@inproceedings{andrew2014global,
  title={{{Global Parameter Estimation for a Eukaryotic Cell Cycle Model
    in Systems Biology}}},
  author={Tricity Andrew and Brandon Amos and David Easterling and Cihan Oguz and
    William Baumann and John Tyson and Layne T. Watson},
  booktitle={Summer Simulation Multiconference,
    Society for Modeling and Simulation International},
  _venue={SummerSim},
  year={2014},
  url={http://dl.acm.org/citation.cfm?id=2685662},
  abstract={
    The complicated process by which a yeast cell divides, known as the cell
    cycle, has been modeled by a system of 26 nonlinear ordinary differential
    equations (ODEs) with 149 parameters. This model captures the chemical
    kinetics of the regulatory networks controlling the cell division process
    in budding yeast cells. Empirical data is discrete and matched against
    discrete inferences (e.g., whether a particular mutant cell lives or dies)
    computed from the ODE solution trajectories. The problem of
    estimating the ODE parameters to best fit the model to the data is a
    149-dimensional global optimization problem attacked by the deterministic
    algorithm VTDIRECT95 and by the nondeterministic algorithms differential
    evolution, QNSTOP, and simulated annealing, whose performances are
    compared.
  }
}

@inproceedings{amos2013applying,
  title={{{Applying machine learning classifiers to dynamic Android
    malware detection at scale}}},
  author={Amos, Brandon and Turner, Hamilton and White, Jules},
  booktitle={IWCMC Security, Trust and Privacy Symposium},
  _venue={IWCMC},
  year={2013},
  codeurl={https://github.com/VT-Magnum-Research/antimalware},
  url={http://bamos.github.io/data/papers/amos-iwcmc2013.pdf},
  abstract={
    The widespread adoption and contextually sensitive
    nature of smartphone devices has increased concerns over smartphone
    malware. Machine learning classifiers are a current method
    for detecting malicious applications on smartphone systems. This
    paper presents the evaluation of a number of existing classifiers,
    using a dataset containing thousands of real (i.e. not synthetic)
    applications. We also present our STREAM framework, which
    was developed to enable rapid large-scale validation of mobile
    malware machine learning classifiers.
  }
}

@article{amos2019limited,
  title={{The Limited Multi-Label Projection Layer}},
  author={Brandon Amos and Vladlen Koltun and J. Zico Kolter},
  journal={arXiv preprint arXiv:1906.08707},
  year={2019},
  _venue={arXiv},
  url={https://arxiv.org/abs/1906.08707},
  codeurl={https://github.com/locuslab/lml},
  abstract={
    We propose the Limited Multi-Label (LML) projection layer as a new
    primitive operation for end-to-end learning systems. The LML layer
    provides a probabilistic way of modeling multi-label predictions
    limited to having exactly k labels. We derive efficient forward and
    backward passes for this layer and show how the layer can be used to
    optimize the top-k recall for multi-label tasks with incomplete label
    information. We evaluate LML layers on top-k CIFAR-100 classification
    and scene graph generation. We show that LML layers add a negligible
    amount of computational overhead, strictly improve the model's
    representational capacity, and improve accuracy. We also revisit the
    truncated top-k entropy method as a competitive baseline for top-k
    classification.
  }
}

@article{grefenstette2019generalized,
  title={{Generalized Inner Loop Meta-Learning}},
  author={Grefenstette, Edward and Amos, Brandon and Yarats, Denis and Htut, Phu Mon and Molchanov, Artem and Meier, Franziska and Kiela, Douwe and Cho, Kyunghyun and Chintala, Soumith},
  journal={arXiv preprint arXiv:1910.01727},
  year={2019},
  _venue={arXiv},
  url={https://arxiv.org/abs/1910.01727},
  codeurl={https://github.com/facebookresearch/higher},
  abstract={
    Many (but not all) approaches self-qualifying as "meta-learning" in
    deep learning and reinforcement learning fit a
    common pattern of approximating the solution to a
    nested optimization problem. In this paper, we give
    a formalization of this shared pattern, which we
    call GIMLI, prove its general requirements, and
    derive a general-purpose algorithm for implementing
    similar approaches. Based on this analysis and
    algorithm, we describe a library of our design,
    higher, which we share with the community to assist
    and enable future research into these kinds of
    meta-learning approaches. We end the paper by
    showcasing the practical applications of this
    framework and library through illustrative
    experiments and ablation studies which they
    facilitate.
  }
}

@techreport{gao2015cloudlets,
  title={Are Cloudlets Necessary?},
  author={
    Gao, Ying and Hu, Wenlu and Ha, Kiryong and
    Amos, Brandon and Pillai, Padmanabhan and
    Satyanarayanan, Mahadev
  },
  _venue={CMU},
  year={2015},
  institution={Technical Report CMU-CS-15-139, CMU School of Computer Science},
  url={http://reports-archive.adm.cs.cmu.edu/anon/anon/2015/CMU-CS-15-139.pdf},
  abstract={
    We present experimental results from Wi-Fi and 4G LTE networks to validate the
    intuition that low end-to-end latency of cloud services improves application
    response time and reduces energy consumption on mobile devices. We focus
    specifically on computational offloading as a cloud service. Using a wide
    range of applications, and exploring both pre-partitioned and dynamically
    partitioned approaches, we demonstrate the importance of low latency for
    cloud offload services. We show the best performance is achieved by
    offloading to cloudlets, which are small-scale edge-located data centers. Our
    results show that cloudlets can improve response times 51\% and reduce energy
    consumption in a mobile device by up to 42\% compared to cloud offload.
  }
}

@techreport{ha2015adaptive,
  title={Adaptive VM handoff across cloudlets},
  author={
    Ha, Kiryong and Abe, Yoshihisa and Chen, Zhuo and
    Hu, Wenlu and Amos, Brandon and Pillai, Padmanabhan and
    Satyanarayanan, Mahadev
  },
  _venue={CMU},
  year={2015},
  institution={Technical Report CMU-CS-15-113, CMU School of Computer Science},
  url={http://ra.adm.cs.cmu.edu/anon/2015/CMU-CS-15-113.pdf},
  abstract={
    Cloudlet offload is a valuable technique for ensuring low end-to-end latency of
    resource-intensive cloud processing for many emerging mobile applications.
    This paper examines the impact of user mobility on cloudlet offload, and
    shows that even modest user mobility can result in significant network
    degradation. We propose VM handoff as a technique for seamlessly transferring
    VM-encapsulated execution to a more optimal offload site as users move. Our
    approach can perform handoff in roughly a minute even over limited WANs by
    adaptively reducing data transferred. We present experimental results to
    validate our implementation and to demonstrate effectiveness of adaptation to
    changing network conditions and processing capacity
  }
}

@inproceedings{brown2018depth,
  title={Depth-Limited Solving for Imperfect-Information Games},
  author={Brown, Noam and Sandholm, Tuomas and Amos, Brandon},
  year={2018},
  booktitle={NeurIPS},
  _venue={NeurIPS},
  url={http://arxiv.org/abs/1805.08195},
  abstract={
A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.
  }
}

@article{wang2018enabling,
  title={Enabling Live Video Analytics with a Scalable and Privacy-Aware Framework},
  author={Wang, Junjue and Amos, Brandon and Das, Anupam and Pillai, Padmanabhan and Sadeh, Norman and Satyanarayanan, Mahadev},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  volume={14},
  _venue={ACM TOMM},
  number={3s},
  pages={64},
  year={2018},
  publisher={ACM},
  abstract={
    We show how to build the components of a privacy-aware, live video
    analytics ecosystem from the bottom up, starting
    with OpenFace, our new open-source face recognition
    system that approaches state-of-the-art
    accuracy. Integrating OpenFace with interframe
    tracking, we build RTFace, a mechanism for
    denaturing video streams that selectively blurs
    faces according to specified policies at full frame
    rates. This enables privacy management for live
    video analytics while providing a secure approach
    for handling retrospective policy
    exceptions. Finally, we present a scalable,
    privacy-aware architecture for large camera networks
    using RTFace and show how it can be an enabler for a
    vibrant ecosystem and marketplace of privacy-aware
    video streams and analytics services.
  },
  url={https://dl.acm.org/citation.cfm?id=3209659}
}

@inproceedings{wang2017scalable,
  title={A Scalable and Privacy-Aware IoT Service for Live Video Analytics},
  author={Wang, Junjue and Amos, Brandon and Das, Anupam and Pillai, Padmanabhan and Sadeh, Norman and Satyanarayanan, Mahadev},
  booktitle={Proceedings of the 8th ACM on Multimedia Systems Conference},
  pages={38--49},
  year={2017},
  organization={ACM},
  url={http://elijah.cs.cmu.edu/DOCS/wang-mmsys2017.pdf},
  codeurl={http://cmusatyalab.github.io/openface/},
  _venue={ACM MMSys},
  _note={Best Paper},
  abstract={
    We present OpenFace, our new open-source face recognition system
    that approaches state-of-the-art accuracy. Integrating OpenFace with
    inter-frame tracking, we build RTFace, a mechanism for denaturing video
    streams that selectively blurs faces according to specified
    policies at full frame rates. This enables privacy management for
    live video analytics while providing a secure approach for handling
    retrospective policy exceptions. Finally, we present a scalable,
    privacy-aware architecture for large camera networks using RTFace.
  }
}


